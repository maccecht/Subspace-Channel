{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2f6b81-897b-457f-ac4f-c0aebf47e837",
   "metadata": {},
   "source": [
    "# Semantic Subspace plotter for people who don't want to do any coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27661fc-a5ba-484c-aa71-1e7bd232ff05",
   "metadata": {},
   "source": [
    "This is all the code you should need to take a text document, train a word2vec model on it, and then search for a semantic subspace in your model in the way outlined in the very cool paper 'Semantic projection recovers rich human knowledge of multiple object features from word embeddings': https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10349641/ . There are various points here where you can change parameters if you want but in theory, anyone should be able to use this.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326c72d-3978-41cc-8abe-61d0b7f73b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For cleaning the data\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#For training the network\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#For representing/reducing the data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import adjustText\n",
    "import seaborn as sns\n",
    "\n",
    "#Because you always need numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceaa28d-d8b9-4f83-bcfb-ae0edd3b05c8",
   "metadata": {},
   "source": [
    "The first bit of code takes the text from your document and does some basic cleaning and tokenisation to make it ready for training the model. \n",
    "All you need to do is plug the address of your document into the open() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9b72c-7afe-4bbe-9a6f-cf4de5d7f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('your_document_here.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "    \n",
    "corpus_lower = corpus.lower()\n",
    "data = [word_tokenize(t) for t in sent_tokenize(corpus_lower)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2b933-f9d1-4716-89a1-ef93f2c06bad",
   "metadata": {},
   "source": [
    "The next six lines of code build the model and then train it on your textual data. Feel free to change the hyperparameters if you feel like it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe70003-d9fe-4762-a518-e3fd353d3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the model\n",
    "model = gensim.models.Word2Vec(window=10, min_count=2, workers=4, negative=10)\n",
    "model.build_vocab(data, progress_per=6000)\n",
    "model.epochs=15\n",
    "model.corpus_count\n",
    "\n",
    "#Train the model\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e629f3c9-bd1d-4d1f-ab14-ed2f2bb30d69",
   "metadata": {},
   "source": [
    "The following two functions create the semantic subspace from your antonyms and project the vectors in a list on to it. \n",
    "The code here is basically taken directly from the original paper except that it has been translated from Matlab into python. \n",
    "The original code is here: https://osf.io/5r2sz/?view_only= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c99bd-47a2-4183-94e0-3496300af568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Semantic subspace plotter. This can be improved by using a dictionary with multiple values rather than taking individual values\n",
    "def get_semantic_subspace(vectors, value_a, value_b):\n",
    "    \"\"\"Computes the semantic subspace for a pair of antonyms\"\"\"\n",
    "    sideA = vectors[value_a]\n",
    "    sideB = vectors[value_b]\n",
    "\n",
    "    # Compute the subspace vector (from SideB to SideA)\n",
    "    theSubspace = sideA - sideB\n",
    "\n",
    "    # Normalize the subspace vector\n",
    "    norm = np.linalg.norm(theSubspace)\n",
    "    if norm == 0:\n",
    "        raise ValueError(\"The subspace vector has zero magnitude.\")\n",
    "    theSubspace = theSubspace / norm\n",
    "    \n",
    "    return theSubspace\n",
    "\n",
    "#Project onto subspace\n",
    "def project_onto_subspace(vectors, subspace):\n",
    "    \"\"\"\n",
    "    Projects your vectors onto the semantic subspace.\n",
    "    Parameters:\n",
    "    - vectors (np.ndarray): A numpy array of shape (n, d) where n is the number of vectors\n",
    "                            and d is the dimensionality of each vector.\n",
    "    - subspace (np.ndarray): The normalized subspace vector.\n",
    "    Returns:\n",
    "    - projections (np.ndarray): A numpy array containing the projections of the vectors onto the subspace.\n",
    "    \"\"\"\n",
    "    if len(vectors.shape) == 1:\n",
    "        # Single vector case, reshape to make it consistent with matrix operations\n",
    "        vectors = vectors.reshape(1, -1)\n",
    "\n",
    "    # Compute the dot product of each vector with the subspace\n",
    "    dot_products = np.dot(vectors, subspace)\n",
    "\n",
    "    # Compute the projections\n",
    "    projections = np.outer(dot_products, subspace)\n",
    "    \n",
    "    return projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2285d-7d3e-459d-a798-31621a0fb5ba",
   "metadata": {},
   "source": [
    "The next function uses Principle Component Analysis and matplotlib to make your results presentable. \n",
    "Honestly, there is probably a much better way to present this data and what comes out is likely to have many of the usual problems you get with matplotlib. \n",
    "The PCA doesn't really do anything since your data has already been projected onto a one dimensionsal space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8a864-e634-42b9-bc87-d4458fa3c5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PCA to reduce those vectors to a two dimensional array in order to make things clearer\n",
    "def plot_2d_representation_of_words(\n",
    "    word_list, \n",
    "    word_vectors,\n",
    "    antonyms,\n",
    "    flip_x_axis = False,\n",
    "    flip_y_axis = False,\n",
    "    label_x_axis = \"meaningless\",\n",
    "    label_y_axis = \"meaningless\", \n",
    "    label_label = \"Semantic Subspace\"):\n",
    "    \n",
    "    pca = PCA(n_components = 2)\n",
    "    \n",
    "    word_plus_coordinates=[]\n",
    "    \n",
    "    for word in word_list: \n",
    "        current_row = []\n",
    "        current_row.append(word)\n",
    "        current_row.extend(word_vectors[word])\n",
    "        word_plus_coordinates.append(current_row)\n",
    "    \n",
    "    word_plus_coordinates = pd.DataFrame(word_plus_coordinates)   \n",
    "    coordinates_2d = pca.fit_transform(\n",
    "        word_plus_coordinates.iloc[:,1:300])\n",
    "    coordinates_2d = pd.DataFrame(\n",
    "        coordinates_2d, columns=[label_x_axis, label_y_axis])\n",
    "    coordinates_2d[label_label] = word_plus_coordinates.iloc[:,0]\n",
    "    if flip_x_axis:\n",
    "        coordinates_2d[label_x_axis] = \\\n",
    "        coordinates_2d[label_x_axis] * (-1)\n",
    "    if flip_y_axis:\n",
    "        coordinates_2d[label_y_axis] = \\\n",
    "        coordinates_2d[label_y_axis] * (-1) \n",
    "    plt.figure(figsize = (60,45))\n",
    "    p1=sns.scatterplot(\n",
    "        data=coordinates_2d, x=label_x_axis, y=label_y_axis)\n",
    "    x = coordinates_2d[label_x_axis]\n",
    "    y = coordinates_2d[label_y_axis]  \n",
    "    antonyms_key = []\n",
    "    for side in antonyms:\n",
    "        antonyms_key.append(word_vectors[side])\n",
    "    plt.plot(antonyms_key, color = 'w')\n",
    "    \n",
    "    label = coordinates_2d[label_label]\n",
    "    texts = [plt.text(x[i], y[i], label[i], family=\"serif\", rotation=90) for i in range(len(x))]\n",
    "    adjustText.adjust_text(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346074c-eb92-446e-8619-f5fad1df65a3",
   "metadata": {},
   "source": [
    "The final chunk of code puts all these functions to work. \n",
    "First, it gets the subspace and then it plots this to a two dimensional image so you can read it. \n",
    "This is where you can plug in the antonyms you want to use. \n",
    "All you have to do to use this is put the words you are interested in into the word_list, and the antonyms you are interested in into the slots for 'antonym1' and 'antonym2' in the two spaces required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1e248-0820-45e2-9c4d-307a3b70122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stick the words in the dataset that you want to examine into the word list. \n",
    "word_list = ['word', 'word1', 'word2', 'word3'...]\n",
    "\n",
    "#Compute the semantic subspace and place it in a dictionary\n",
    "output_projections = project_onto_subspace(vectors, get_semantic_subspace(vect_dict, \"antonym1\", \"antonym2\"))\n",
    "output_tuple = [(key, value) for i, (key, value) in enumerate(zip(keys, output_projections))]\n",
    "output_dict = dict(output_tuple)\n",
    "\n",
    "#Print that two dimensional array\n",
    "phil_map = plot_2d_representation_of_words(\n",
    "    word_list = word_list, \n",
    "    word_vectors = output_dict, \n",
    "    antonyms = ['antonym1', 'antonym2'],\n",
    "    flip_y_axis = False,\n",
    "    flip_x_axis = False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
